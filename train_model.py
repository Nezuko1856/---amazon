import pandas as pd
import kagglehub
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import pickle
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import os
import numpy as np

# –°–∫–∞—á–∏–≤–∞–µ–º NLTK –¥–∞–Ω–Ω—ã–µ
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)



print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

# –ï—Å–ª–∏ kagglehub –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
try:
    path = kagglehub.dataset_download("arhamrumi/amazon-product-reviews")
    df = pd.read_csv(path + '/Reviews.csv')
    print(f" –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")
except Exception as e:
    print(f" –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
    print(" –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç...")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    np.random.seed(42)
    n_samples = 50000
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
    positive_templates = [
        "Excellent product! {feature} works perfectly.",
        "Very satisfied with my purchase. The {feature} is amazing.",
        "Great value for money. {feature} exceeded my expectations.",
        "Highly recommend! {feature} is top quality.",
        "Best purchase ever. The {feature} is fantastic.",
        "Love this product! {feature} is excellent.",
        "Amazing quality, very happy with {feature}.",
        "Perfect product for my needs. {feature} works great.",
        "Would buy again. {feature} is reliable and durable.",
        "Excellent performance from {feature}. Highly satisfied."
    ]
    
    negative_templates = [
        "Terrible product. {feature} broke immediately.",
        "Very disappointed. The {feature} doesn't work properly.",
        "Waste of money. {feature} is poor quality.",
        "Do not recommend. {feature} failed after 2 days.",
        "Worst purchase. {feature} is defective.",
        "Poor quality product. {feature} stopped working.",
        "Not worth the price. {feature} has issues.",
        "Very unhappy with {feature}. Doesn't meet expectations.",
        "Would not buy again. {feature} is unreliable.",
        "Disappointing performance from {feature}."
    ]
    
    features = ['battery', 'screen', 'camera', 'performance', 'sound', 
                'build quality', 'software', 'design', 'price', 'delivery']
    
    reviews = []
    scores = []
    
    for i in range(n_samples):
        if np.random.random() > 0.3:  # 70% positive, 30% negative
            template = np.random.choice(positive_templates)
            score = 5  # –í—Å–µ positive –ø–æ–ª—É—á–∞—é—Ç 5 –∑–≤—ë–∑–¥
        else:
            template = np.random.choice(negative_templates)
            score = 1  # –í—Å–µ negative –ø–æ–ª—É—á–∞—é—Ç 1 –∑–≤–µ–∑–¥—É
            
        feature = np.random.choice(features)
        review = template.format(feature=feature)
        
        reviews.append(review)
        scores.append(score)
    
    df = pd.DataFrame({'Text': reviews, 'Score': scores})
    print(f" –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")

# –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50–∫ –∑–∞–ø–∏—Å–µ–π
if len(df) > 50000:
    df = df.sample(n=50000, random_state=42)
    print(f" –ò—Å–ø–æ–ª—å–∑—É–µ–º 50,000 —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞")
else:
    print(f" –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ {len(df)} –∑–∞–ø–∏—Å–µ–π")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = df[['Text', 'Score']].dropna()
df['Sentiment'] = df['Score'].apply(lambda x: 'positive' if x > 3 else 'negative')

print(df['Sentiment'].value_counts())
print(f"Positive: {len(df[df['Sentiment'] == 'positive']):,} ({len(df[df['Sentiment'] == 'positive'])/len(df)*100:.1f}%)")
print(f"Negative: {len(df[df['Sentiment'] == 'negative']):,} ({len(df[df['Sentiment'] == 'negative'])/len(df)*100:.1f}%)")

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
    text = re.sub(r'[^a-zA-Z\s]', '', str(text))
    text = text.lower()
    words = text.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return ' '.join(words)

print("\nüîÑ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
df['Processed_Text'] = df['Text'].apply(preprocess_text)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X_train, X_test, y_train, y_test = train_test_split(
    df['Processed_Text'], df['Sentiment'], 
    test_size=0.2, 
    random_state=42,
    stratify=df['Sentiment']
)


print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train):,} –∑–∞–ø–∏—Å–µ–π")
print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test):,} –∑–∞–ø–∏—Å–µ–π")

# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è

vectorizer = TfidfVectorizer(
    max_features=2000,
    min_df=5,
    max_df=0.7,
    ngram_range=(1, 2)
)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {X_train_vec.shape[1]} features")

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
label_encoder = LabelEncoder()
y_train_enc = label_encoder.fit_transform(y_train)
y_test_enc = label_encoder.transform(y_test)

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏

model = Sequential([
    Dense(256, activation='relu', input_shape=(X_train_vec.shape[1],)),
    Dropout(0.4),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.1),
    Dense(1, activation='sigmoid')  # –û–¥–∏–Ω –≤—ã—Ö–æ–¥ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
)

print(model.summary())

# Callbacks
callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001)
]

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

history = model.fit(
    X_train_vec.toarray(), y_train_enc,
    epochs=5,
    batch_size=32,
    validation_data=(X_test_vec.toarray(), y_test_enc),
    callbacks=callbacks,
    verbose=1
)

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
print("\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
test_loss, test_accuracy, test_precision, test_recall = model.evaluate(
    X_test_vec.toarray(), y_test_enc, verbose=0
)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –æ—Ç—á–µ—Ç–∞
y_pred = (model.predict(X_test_vec.toarray()) > 0.5).astype(int)


print(f"–¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-Score: {2 * test_precision * test_recall / (test_precision + test_recall):.4f}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞

model.save('neural_model.keras')

with open('vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)

with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
with open('training_history.pkl', 'wb') as f:
    pickle.dump(history.history, f)


test_samples = [
    ("This product is absolutely amazing! Best purchase I've made all year.", "positive"),
    ("Terrible quality, broke after just 2 days of use.", "negative"),
    ("Good value for the price, works as expected.", "positive"),
    ("Worst product ever, complete waste of money.", "negative"),
    ("The camera quality is excellent and battery lasts all day.", "positive"),
    ("BUY NOW!!! 50% OFF LIMITED TIME 12345 $$$", "spam"),
    ("Excellent service and fast delivery. Highly recommended!", "positive"),
    ("Doesn't work properly, very disappointed with this purchase.", "negative")
]

for text, expected in test_samples:
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º
    is_spam = False
    spam_reason = ""
    
    if len(text.split()) < 3:
        is_spam = True
        spam_reason = "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π"
    elif sum(c.isdigit() for c in text) / len(text) > 0.3:
        is_spam = True
        spam_reason = "–ú–Ω–æ–≥–æ —Ü–∏—Ñ—Ä"
    elif 'BUY NOW' in text.upper() or '$$$' in text or '!!!' in text:
        is_spam = True
        spam_reason = "–†–µ–∫–ª–∞–º–Ω—ã–π —Ç–µ–∫—Å—Ç"
    
    if is_spam:
        print(f" –¢–µ–∫—Å—Ç: {text[:50]}...")
        print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π: spam (95.00%)")
        print(f"  –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–ø–∞–º: {spam_reason}")
    else:
        processed = preprocess_text(text)
        vectorized = vectorizer.transform([processed])
        prediction = model.predict(vectorized.toarray(), verbose=0)[0][0]
        sentiment = 'positive' if prediction > 0.5 else 'negative'
        confidence = prediction if sentiment == 'positive' else 1 - prediction
        
        print(f"   –û–∂–∏–¥–∞–µ–º—ã–π: {expected}, –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π: {sentiment} ({confidence:.2%})")
    
